---
layout: post
title: Model Inference at Edge
comments: true
published: false
hidden: true
---

Our hyper-excitement from AI blasts, as if coming from the future, is often stopped at how to bring that excitement to every corner of the daily life. Compared to massive clusters with powerful A100 NVIDIA GPU cards, end-users PC are equipped with hardware from the 20-th century; on smartphones the situation is not getting any better with additional constraints on battery and connectivity. Not only the technicality aspect of the problem, but also the economic aspect as well. Running inference costs dollars and cloud-based clusters aren't free or cheap. The commoditization of AI eventually boils down as a producer-consumer problem of the free market: there must be incentives for AI to be used, for example to improve productivity, or security, or welfare, and in exchange of its deployment is cheap enough.

This is no surprise of why the projection on AI trend bets heavily on the market of AI inference and not model training. Cloud-based inference could be made cheaper; in the long run for any AI-based applications that generates steady data streams, the cloud-based solution is becoming expensive enough to bite off a big chunk of profit margin. Paying an upfront cost to have AI inference running on edge devices is a worthy investment. Edge deployment however is not easy; it is constrained by many factors and requires deployers to navigate trade-off space of cost, speed, accuracy, model compatibility, backward-compatibility, maintenance.

There are progresses. For the majority of cases, edge inference moves away from expensive GPU architectures such as of NVIDIA. It is powerful but expensive for one hand and power hungry on the other hand. That does not mean NVIDIA is a weak player in this market. On the contrary they are working hard to deliver budget products for the edge market too. Manufacturers other than NVIDIA including Qualcomm, Intel, ARM focus con less power-consumming chips. For the moment this sector has been behind NVIDIA's lead but I believe in the years to come there will be a few winners standing out.

The flagship edge inference that comes from NVIDIA is Jetson family featuring TX and TX2 for several years and show shifting their focus to the new architecture Orin. Jetson Orin Nano is their next bet in the second half of 2020s. From Google there is Coral featuring a variety of ML accelerated devices in different form factors. Basically it is similar to Raspberry Pi but with accelerated hardware for deep learning; it is similar to Jetson's in this regard but uses Google's TPU (Tensor Processing Unit) instead of GPU (Graphics Processing Unit). It is claimed that TPU gives faster inference speed however its unique architecture is also its limitation. Jetson cards on the contrary can be used to run different models trained from different platforms such as Tensorflow and Pytorch as long as the CUDA library is used. That is to say, Coral boards are specifically designed to run Tensorflow Lite models and no other platforms at least in the foreseable future.

With the flourishing of many deep learning libraries to train and run neural network models, including Tensorflow, Pytorch, Mxnet, it is difficult to unify the way operators are defined and run. On the other hand, industrial standardisation of model serialization is required for seamless deployment to different inference devices. It is worth to remind ourselves that by the end of the day neural networks are no magical than a huge graph of computation nodes and edges between them defines how inputs and outputs are connected. Under this light it is possible to iron out the starky differences between training and inference steps, as well as mathematics operations supported by different hardware. ONNX a.k.a Open Neural Network Exchange comes to this world for that only reason. The stadardisation, supported by many big corps including Microsoft, should be a prefered way to exchange models and weights trained by different tools. Hardware vendors also rely on ONNX's definition of supported operations, called Opset, to speak a common language. The real world nevertheless is never close to an utopia which leads to the fact that currently there are many standards coexists. 

At its heart, ONNX defines standards and by no mean specific implementation on particular hardware. Hardware vendors themselves with their proprietary acceleration libraries convert ONNX operations into machine-understood instructions. In the remainder of this post I focus 
