---
layout: post
title: Multimodal Large Language Model
comments: true
published: false
hidden: true
---

# Introduction

Multimodal inference, or machine learning on the fusion of data of different kinds including images, languages, musics, sounds, videos, logically the next step of AI advancement. It is not that LLMs (Large Language Models) have achieved super accuracies within the text domain, but combining different kinds of data, especially done in a native way (end-to-end), are vital to unlock more spectacular capabilities of large models. Multimodal data bridge the gaps of realities when brought into the world of multi-dimensional AI. On one hand it leverage the abundant amount of data still there from the Internet left unlearned; on the other hand AI models are given a chance to connect latent concepts learned from different subspaces of for example images and text. There is no second-thought in this front, just that how far we can get from them. Our best hope is the ability to simulate the physical world by mastering all these data.

# Literature review

A great point to start is reading literature review papers, among which this ["MME: A comprehensive evaluation benchmark for multimodal large language models"](https://arxiv.org/pdf/2306.13394) is a good read. According to the paper, MLLM is the next phase advancement of LLM where multimodal information is used to perform perception and cognition/reasoning tasks. Three representative abilities of LLMs still exist and developed further with MLLMs, which are:

- Instruction folowing
- In-context learning
- Chain of thought

Since these seemingly emergent abilities make what people think of LLMs being intelligent, a sensible surgery approach to MLLM is to fully understand how these abilities were trained, and from what data. We will go one by one.

## Instruction following


# Open Souce State of the Art



